{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4100857,"sourceType":"datasetVersion","datasetId":2425019},{"sourceId":8044617,"sourceType":"datasetVersion","datasetId":4743359},{"sourceId":10003598,"sourceType":"datasetVersion","datasetId":6157722},{"sourceId":10073854,"sourceType":"datasetVersion","datasetId":6209459},{"sourceId":10126916,"sourceType":"datasetVersion","datasetId":6249388}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U nltk[twitter]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#pip install -U nltk[all]","metadata":{"_uuid":"973249e5-5b9a-49e6-85d8-703c680f84e3","_cell_guid":"3b652d6d-7746-4d99-b573-c472a71d1e63","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport datetime\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk\nfrom nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\n\n#df.sample()\npd.set_option(\"display.max_colwidth\", 1500)\npd.set_option(\"display.max_rows\", 10)\n\ndf=pd.read_csv(r'../input/<twitter_path/file_name>.csv')\n\n#print(dir(df))\ndf.sample()","metadata":{"_uuid":"b259743d-daa0-4dbe-ae5f-ab480702235d","_cell_guid":"03cb8dc3-cab6-4b0f-8413-6c5f8bc34239","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Date\"]=pd.to_datetime(df[\"Datetime\"],format='%Y-%m-%d %H:%M:%S+00:00').dt.strftime('%Y-%m-%d')\ndf[\"DOY\"]=pd.to_datetime(df[\"Datetime\"],format='%Y-%m-%d %H:%M:%S+00:00').dt.dayofyear\ndf[\"Month\"]=pd.to_datetime(df[\"Datetime\"],format='%Y-%m-%d %H:%M:%S+00:00').dt.month\ndf[\"Quarter\"]=pd.to_datetime(df[\"Datetime\"],format='%Y-%m-%d %H:%M:%S+00:00').dt.quarter\ndf[\"Year\"]=pd.to_datetime(df[\"Datetime\"],format='%Y-%m-%d %H:%M:%S+00:00').dt.year\ndf.head(2)","metadata":{"_uuid":"119ac2a2-7e68-4548-a154-9855770798e8","_cell_guid":"42743ba6-a298-4760-a52b-0e6b8ae82b2f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(\"Tweet Id\",keep=False, inplace=True)","metadata":{"_uuid":"69de2ae7-dc2c-4b2f-8143-9c96a8daaa52","_cell_guid":"8446ee67-4b48-45ab-96ea-2cbd4681b912","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langdetect import detect","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#detect language of the tweet and populate a new language column\ndf[\"Language\"]=df[\"Text\"].apply(detect)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#find  all synonyms for aspect mining and check them all later.\nfrom nltk.corpus import wordnet\n\ndef get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonyms.add(lemma.name())\n    return synonyms\n\nword = 'inflation'\nsynonyms = get_synonyms(word)\nprint(f\"Synonyms of '{word}': {synonyms}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All the words found above are manually checked as a oneoff during the exercise. \nno match found of any of the synonym for inflation or interest rate\nword rate was also used but didnt make any much difference.","metadata":{}},{"cell_type":"code","source":"df[\"AspectInflation\"]=df['Text'].str.lower().replace(' ', '').str.contains('inflation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"AspectIntestRate\"]=df['Text'].str.lower().replace(' ', '').str.contains('interestrate')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Employment\"]=df['Text'].str.lower().replace(' ', '').str.contains('employment')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#apply aspect mining code and populate aspect as separate column in dataframe\n#aspect mining code\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt_tab')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Remove stopwords\nstop_words = set(stopwords.words('english'))\n\n# Lemmatization\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(r'/kaggle/working/tmp_combined_data.csv')","metadata":{"_uuid":"d78bf812-ceab-4076-9c4b-e84aa12b594f","_cell_guid":"1564776f-2930-4a58-841d-f8ee4a9f0372","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#filterout 6 records from other languages which are es->Spanish , de-> German , and zh-ch -> Chinese\n#language RO-> romanian has english words, so not filtering it\nlen(df)\nlanguages_to_exclude=['es','de','zh-cn']\ndf=df[~df['Language'].isin(languages_to_exclude)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Named entity recognition tags/codes.**\n\nJJ (adjective): Adjectives describe or modify nouns or pronouns, providing additional information about their qualities, characteristics, or attributes. In the context of NER, adjectives may not directly represent named entities themselves but can provide context or additional information about entities.\n\nNN (noun): Nouns represent entities such as people, places, objects, or concepts. In NER, common nouns may or may not represent named entities, depending on context.\n\nNNP (proper noun, singular): Proper nouns typically represent specific entities such as names of people, organizations, or locations. Proper nouns are often the primary focus of NER.\n\nNNPS (proper noun, plural): Similar to NNP, NNPS represents plural proper nouns.\n\nNNS (noun, plural): Plural nouns represent multiple instances of entities, objects, or concepts.","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk\n\nnltk.download('averaged_perceptron_tagger_eng')\nnltk.download('maxent_ne_chunker_tab')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_person_place(text, label,debug_mode=False):\n    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n    if debug_mode:\n        print(\"chunked is\" +str(chunked)+ ' and count of chunked is '+str(len(chunked)))\n    #print(chunked[0])\n    #print('debug 1')\n    #print(chunked[1])\n    #print('debug 2')\n    prev = None\n    continuous_chunk = []\n    current_chunk = []\n    for subtree in chunked:\n        if type(subtree) == Tree:\n            if subtree.label() in label:\n                if debug_mode:\n                    print('debug1'+str(subtree))\n                continue\n            else:\n                if debug_mode:\n                    print('debug2'+str(subtree))\n                continuous_chunk.append(\"\".join([token for token, pos in subtree.leaves()]))\n        else:\n           if debug_mode:\n            print('debug 3:'+str(subtree))\n           #continuous_chunk.append(\" \".join([token for (token, pos) in subtree]))\n           continuous_chunk.append(\"\".join(subtree[0]))\n    return ' '.join(continuous_chunk)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#download stopwords from corpus\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nprint(stop)","metadata":{"_uuid":"eabb7eb7-81fa-4531-a301-2ae619de4348","_cell_guid":"c71adcfa-c837-4b76-9525-fd614464b859","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#get list of australian postcodes and convert to a list, so it can be filtered out from the main twitter dataframe content.\n#This filter is working. this might be a candidate to be added to the list of stop words.\n#df_au_postcodes=pd.read_csv(r\"https://github.com/schappim/australian-postcodes/blob/master/australian-postcodes-2021-04-23.csv\")\ndf_au_postcodes=pd.read_csv(r'../input/au-postcodes/australian-postcodes-2021-04-23.csv')\nau_postcodes_list=df_au_postcodes[\"Suburb\"].str.lower().tolist()\n#au_postcodes_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#text cleaning block.\n#remove stop words. This will convert each line into a list of words\n#filterout suburb names by checking if the word is appearing in au_post_codes_list. limited filtering only based on direct matching.\n#ignore words with 2 or less characters\n#ignore word is not a pure number\n#remove apostrophe from words e.g.  '#rba', vs  '#rbaâ€™',\n#remove http/s links from the text\n#remove handles like @theblogger\n#remove ampersand characters like '&amp;'\n#including block to filter all hashtags e.g. #BRIKdolphin\nremove_hashtags = lambda x: re.sub(r'#\\S+', '', x)\nremove_links = lambda x: re.sub(r'http\\S+', '', x)\nremove_handles = lambda x:re.sub('@[^\\s]+','',x)\nremove_ampersands = lambda x:re.sub(r'&amp;','',x)\nremove_special_chars = lambda x: re.sub('[!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@â€˜#]', '', x)\ndf[\"text_modified\"]=df[\"Text\"].apply(lambda line:[remove_special_chars(remove_handles(remove_ampersands(remove_links(remove_hashtags(word.lower())))))\n                                                           for word in line.split()\n                                                           if word.lower() not in stop and len(word) > 2 and re.match('^[0-9]+$',word) is None])\ndf[\"text_modified\"]=df[\"text_modified\"].apply(lambda line:[word\n                                                           for word in line\n                                                           if re.match('^[0-9]+$',word) is None])\ndf[\"text_modified\"]=df[\"text_modified\"].apply(lambda line:[word\n                                                           for word in line\n                                                           if word not in au_postcodes_list])\ndf[\"text_modified\"]","metadata":{"_uuid":"41f59a49-4b80-47d1-9a86-9730a525f484","_cell_guid":"4d7391b3-77c6-4e27-9ae8-8dd08ec2bb32","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"text_modified\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#remove all non alphabetic characters e.g. emoji, star, thumsup etc\nremove_non_alphabetic_characters = lambda x: re.sub(r'[^A-Za-z0â€“9]', '', x)\ndf[\"text_modified\"]=df[\"text_modified\"].apply(lambda line:[remove_non_alphabetic_characters(word)\n                                                           for word in line])\ndf[\"text_modified\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\n#nltk.download()\nnltk.download('omw-1.4')","metadata":{"_uuid":"f4b55def-1240-4bcf-be5d-71b9d1437fd8","_cell_guid":"8a791fca-7116-4780-a151-b65d736e9d8b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"text_modified_string\"]=df[\"text_modified\"].apply(lambda x : ' '.join(x))","metadata":{"_uuid":"bcc94221-f297-4ffa-83b1-78bb87f69ab2","_cell_guid":"9c755621-0b7c-477b-a753-56dac2e01cf3","jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this is done after cleaning the data now, since it was introducing spaces in the hyperlink and causing problems while filtering all hyperlinks.\n#DO NOT CHANGE the position of this transformer\ndf[\"text_modified_string\"]=df[\"text_modified_string\"].apply(lambda line:filter_person_place(line,['GPE','PERSON']))\ndf[\"text_modified_string\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(r'/kaggle/working/tmp_cleaned_data.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_df=1, min_df=1, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n\n# Get distinct year and quarter combinations\ndistinct_year_quarters = df[['Year', 'Quarter']].drop_duplicates()\nprint (distinct_year_quarters)\n\n# Loop through each year-quarter combination\nfor _, row in distinct_year_quarters.iterrows():\n    #print('debug 1'+str(row))\n    year = row['Year']\n    quarter = row['Quarter']\n    \n    # Filter records for the current year and quarter\n    subset = df[(df['Year'] == year) & (df['Quarter'] == quarter)]\n    \n    # Process the subset of records\n    print(f\"Processing records for Year: {year}, Quarter: {quarter}\")\n    #print(subset[\"text_modified_string\"])\n    tf = vectorizer.fit_transform(subset[\"text_modified_string\"].dropna()).toarray()\n    tf_feature_names = vectorizer.get_feature_names_out()\n\n    from sklearn.decomposition import LatentDirichletAllocation\n    number_of_topics = 10\n    model = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n    # Fit data to model\n    model.fit(tf)\n\n    no_top_words = 20\n    #compile display_topics function def  given in cell below\n    topics=display_topics(model, tf_feature_names, no_top_words)\n    topics\n\n    topics.to_csv(r'/kaggle/working/generated_topics_50_year_'+str(year)+'_quarter_'+str(quarter)+'.csv')\n    \n    \n    # Add your processing logic here\n    # For example, save to a separate file or perform some operations\n    # subset.to_csv(f'output_{year}_Q{quarter}.csv', index=False)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# the vectorizer object will be used to transform text to vector form\n#setting max_df and min_df to default values of 1 to not ignore any values.\nvectorizer = CountVectorizer(max_df=1, min_df=1, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n#regex word string may contain $ or . with number and is a non empty string\n#above regex need to be changed to pick all words not starting with a number or do this filtering earlier.\n#vectorizer = CountVectorizer(token_pattern='^[^0-9]+\\w.$')\n\n\n# apply transformation\n#tf = vectorizer.fit_transform(df[\"text_stemmed_string\"]).toarray()#commenting after skipping stemming and lemmatization\ntf = vectorizer.fit_transform(df[\"text_modified_string\"]).toarray()\n\n# tf_feature_names tells us what word each column in the matric represents\ntf_feature_names = vectorizer.get_feature_names_out()","metadata":{"_uuid":"e608e467-6abe-4a49-9af3-2a3e40337cbb","_cell_guid":"b72333ce-2e61-4519-9733-7c3e6bce20d6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#these are the predominant topics\ntf_feature_names","metadata":{"_uuid":"d73ea5f1-f378-4375-98a7-f485168352ef","_cell_guid":"5ecd84cb-e91f-492d-9ede-7bcaa0e1b49b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(tf_feature_names)\n#len(set(tf_feature_names))\n#tf_feature_names","metadata":{"_uuid":"9679ce6b-2b30-4763-982f-92f9db5e539a","_cell_guid":"b2e49c55-5e61-4636-a3ea-3150dd928664","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#filter out links and special characters\n#tf_feature_names\ntf","metadata":{"_uuid":"e6b4a449-729b-4861-abe1-98f655af0f20","_cell_guid":"d139d844-24cb-40c5-8245-bf9446bd993e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.shape","metadata":{"_uuid":"9d985404-a25e-4764-9492-9e9f1dc91440","_cell_guid":"032b6fe0-4f5b-4152-8bac-35281adb0965","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nnumber_of_topics = 10\nmodel = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n# Fit data to model\nmodel.fit(tf)","metadata":{"_uuid":"89689eb7-01c6-4d68-996c-3a05c1a16f4b","_cell_guid":"6785cb77-f819-4600-a10b-c78c069fa59c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    topic_dict = {}\n    for topic_idx, topic in enumerate(model.components_):\n        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n    return pd.DataFrame(topic_dict)","metadata":{"_uuid":"4f0bb639-2c83-4622-ae91-f342afb62dee","_cell_guid":"7fc5ffa7-e3de-4077-90fd-fb5e559e3c54","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_top_words = 50\ntopics=display_topics(model, tf_feature_names, no_top_words)\ntopics","metadata":{"_uuid":"296cb245-0682-47b3-8e85-07d5ea81d716","_cell_guid":"aaa8bbc6-d349-41ec-884a-5d473136b681","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topics.to_csv(r'/kaggle/working/generated_topics_50.csv')","metadata":{"_uuid":"635bab5c-75ec-488a-b2ab-b8e3b7718085","_cell_guid":"2269878e-8649-4d1f-80e3-e2896f1ad937","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#now only pick the records where aspect mining is detected and then apply thematic analysis.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# the vectorizer object will be used to transform text to vector form\n#setting max_df and min_df to default values of 1 to not ignore any values.\nvectorizer = CountVectorizer(max_df=1, min_df=1, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n#regex word string may contain $ or . with number and is a non empty string\n#above regex need to be changed to pick all words not starting with a number or do this filtering earlier.\n#vectorizer = CountVectorizer(token_pattern='^[^0-9]+\\w.$')\n\n\n# apply transformation\n#tf = vectorizer.fit_transform(df[\"text_stemmed_string\"]).toarray()#commenting this and adding next line because we skipped stemming and lemmatization\ntf = vectorizer.fit_transform(df[(df['AspectInflation'] == True) | (df['AspectIntestRate'] == True) | (df['Employment'] == True)][\"text_modified_string\"].dropna()).toarray()\n\n# tf_feature_names tells us what word each column in the matric represents\ntf_feature_names = vectorizer.get_feature_names_out()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#these are the predominant topics\ntf_feature_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(tf_feature_names)\n#len(set(tf_feature_names))\n#tf_feature_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#filter out links and special characters\n#tf_feature_names\ntf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nnumber_of_topics = 10\nmodel = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n# Fit data to model\nmodel.fit(tf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    topic_dict = {}\n    for topic_idx, topic in enumerate(model.components_):\n        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n    return pd.DataFrame(topic_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_top_words = 50\ntopics=display_topics(model, tf_feature_names, no_top_words)\ntopics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topics.to_csv(r'/kaggle/working/generated_topics_50_after_aspect_mining.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}